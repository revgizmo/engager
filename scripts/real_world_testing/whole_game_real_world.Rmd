---
title: "Whole Game: Real-World Workflow for Instructors"
author: "zoomstudentengagement package"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_document:
    toc: true
    toc_float: true
    code_folding: hide
    theme: cosmo
    highlight: tango
    df_print: paged
    encoding: UTF-8
params:
  instructor_name: "Conor Healy"
  allow_lookup_write: false
  course_id: "CS101"
  semester: "Fall 2024"
  privacy_level: "ferpa_standard"
  incremental_mode: true
---

```{r setup, include = FALSE}
# Set encoding for R Markdown
Sys.setlocale("LC_ALL", "en_US.UTF-8")
options(encoding = "UTF-8")

knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 8,
  fig.height = 6,
  warning = FALSE,
  message = FALSE,
  echo = TRUE
)

# Initialize workflow state
workflow_state <- list(
  start_time = Sys.time(),
  course_id = params$course_id,
  semester = params$semester,
  instructor_name = params$instructor_name,
  privacy_level = params$privacy_level,
  incremental_mode = params$incremental_mode,
  processed_sessions = c(),
  errors = c(),
  warnings = c()
)

# Progress tracking function
log_progress <- function(message, type = "info") {
  timestamp <- format(Sys.time(), "%H:%M:%S")
  cat(sprintf("[%s] %s: %s\n", timestamp, toupper(type), message))
  workflow_state$messages <- c(workflow_state$messages, 
                              list(list(time = timestamp, type = type, message = message)))
}

# Error handling function
handle_error <- function(error, context = "Unknown") {
  error_msg <- sprintf("Error in %s: %s", context, error$message)
  log_progress(error_msg, "error")
  workflow_state$errors <- c(workflow_state$errors, error_msg)
  warning(error_msg)
}
```

```{r load-packages}
# Load required packages with error handling
required_packages <- c("zoomstudentengagement", "dplyr", "ggplot2", "readr", "tibble", "lubridate", "jsonlite")

for (pkg in required_packages) {
  tryCatch({
    library(pkg, character.only = TRUE)
    log_progress(sprintf("Loaded package: %s", pkg), "info")
  }, error = function(e) {
    handle_error(e, sprintf("Loading package %s", pkg))
    stop(sprintf("Failed to load required package: %s", pkg))
  })
}
```

# Whole Game: Real-World Workflow for Instructors

This document provides a comprehensive workflow for analyzing student engagement in Zoom sessions. It's designed to work seamlessly for both first-time users and ongoing semester analysis.

## üöÄ Quick Start (First-Time Users)

If this is your first time using this workflow:

1. **Set your parameters** at the top of this file (instructor name, course ID, etc.)
2. **Run the setup validation** in Step 1
3. **Add your data files** as guided in Step 2
4. **Run the workflow** - it will guide you through each step

## üìã Workflow Overview

This workflow provides:
- ‚úÖ **Setup validation** - Ensures your environment is ready
- ‚úÖ **Incremental processing** - Only processes new sessions
- ‚úÖ **Progress tracking** - Shows what's happening and how long it will take
- ‚úÖ **Error recovery** - Handles issues gracefully with clear guidance
- ‚úÖ **Privacy compliance** - FERPA-safe by default
- ‚úÖ **State management** - Remembers what you've already processed

## Step 1: Environment Setup and Validation

Let's start by validating your environment and setting up the required structure:

```{r environment-setup}
log_progress("Starting environment setup and validation", "info")

# Function to create directory if it doesn't exist
ensure_directory <- function(path) {
  if (!dir.exists(path)) {
    dir.create(path, recursive = TRUE, showWarnings = FALSE)
    log_progress(sprintf("Created directory: %s", path), "info")
  }
}

# Function to validate file existence
validate_file <- function(file_path, description) {
  if (file.exists(file_path)) {
    log_progress(sprintf("‚úÖ Found %s: %s", description, basename(file_path)), "success")
    return(TRUE)
  } else {
    log_progress(sprintf("‚ö†Ô∏è  Missing %s: %s", description, file_path), "warning")
    return(FALSE)
  }
}

# Create required directory structure
required_dirs <- c(
  "data",
  "data/metadata", 
  "data/transcripts",
  "data/transcripts/processed",
  "outputs",
  "reports",
  "logs"
)

for (dir in required_dirs) {
  ensure_directory(dir)
}

# Check for required files
log_progress("Validating required files...", "info")

roster_exists <- validate_file("data/metadata/roster.csv", "student roster")
lookup_exists <- validate_file("data/metadata/section_names_lookup.csv", "name lookup file")

# Check for transcript files
transcript_files <- list.files(
  "data/transcripts",
  pattern = "\\.transcript\\.vtt$",
  full.names = TRUE,
  recursive = TRUE
)

if (length(transcript_files) > 0) {
  log_progress(sprintf("‚úÖ Found %d transcript files", length(transcript_files)), "success")
  for (file in transcript_files) {
    log_progress(sprintf("  - %s", basename(file)), "info")
  }
} else {
  log_progress("‚ö†Ô∏è  No transcript files found in data/transcripts/", "warning")
}

# Create session tracking file if it doesn't exist
session_tracking_file <- "data/metadata/session_tracking.csv"
if (!file.exists(session_tracking_file)) {
  session_tracking <- data.frame(
    session_id = character(),
    file_path = character(),
    processed_date = character(),
    status = character(),
    stringsAsFactors = FALSE
  )
  write.csv(session_tracking, session_tracking_file, row.names = FALSE)
  log_progress("Created session tracking file", "info")
}

# Load or create workflow configuration
config_file <- "data/metadata/workflow_config.json"
if (file.exists(config_file)) {
  config <- jsonlite::fromJSON(config_file)
  log_progress("Loaded existing workflow configuration", "info")
} else {
  config <- list(
    course_id = params$course_id,
    semester = params$semester,
    instructor_name = params$instructor_name,
    privacy_level = params$privacy_level,
    incremental_mode = params$incremental_mode,
    created_date = as.character(Sys.Date()),
    last_updated = as.character(Sys.Date())
  )
  jsonlite::write_json(config, config_file, pretty = TRUE)
  log_progress("Created new workflow configuration", "info")
}

log_progress("Environment setup completed", "success")
```

## Step 2: Data Preparation and Validation

Now let's validate and prepare your data files:

```{r data-preparation}
log_progress("Starting data preparation and validation", "info")

# Validate roster file
if (roster_exists) {
  tryCatch({
    roster <- load_roster(data_folder = "data/metadata", roster_file = "roster.csv")
    log_progress(sprintf("‚úÖ Loaded roster with %d students", nrow(roster)), "success")
    
    # Validate roster structure
    required_roster_cols <- c("preferred_name", "formal_name", "student_id")
    missing_cols <- setdiff(required_roster_cols, names(roster))
    
    if (length(missing_cols) > 0) {
      log_progress(sprintf("‚ö†Ô∏è  Roster missing columns: %s", paste(missing_cols, collapse = ", ")), "warning")
    } else {
      log_progress("‚úÖ Roster structure is valid", "success")
    }
    
    # Show roster preview
    knitr::kable(head(roster), caption = "Student Roster Preview")
    
  }, error = function(e) {
    handle_error(e, "Loading roster")
    log_progress("Creating sample roster template...", "info")
    
    # Create sample roster
    sample_roster <- data.frame(
      preferred_name = c("Student One", "Student Two", "Student Three"),
      formal_name = c("Student One", "Student Two", "Student Three"),
      student_id = c("STU001", "STU002", "STU003"),
      stringsAsFactors = FALSE
    )
    
    write.csv(sample_roster, "data/metadata/roster.csv", row.names = FALSE)
    log_progress("Created sample roster template - please edit with your actual student data", "warning")
  })
} else {
  log_progress("No roster file found - please add data/metadata/roster.csv", "warning")
}

# Validate or create lookup file
if (lookup_exists) {
  tryCatch({
    lookup_data <- read_lookup_safely("data/metadata/section_names_lookup.csv")
    log_progress(sprintf("‚úÖ Loaded lookup file with %d entries", nrow(lookup_data)), "success")
    
    # Check for instructor configuration
    instructor_entries <- lookup_data[lookup_data$participant_type == "instructor", ]
    if (nrow(instructor_entries) > 0) {
      log_progress("‚úÖ Instructor configuration found", "success")
      knitr::kable(instructor_entries[, c("transcript_name", "preferred_name", "participant_type")],
                   caption = "Configured Instructors")
    } else {
      log_progress("‚ö†Ô∏è  No instructor configuration found", "warning")
    }
    
  }, error = function(e) {
    handle_error(e, "Loading lookup file")
  })
} else {
  log_progress("No lookup file found - will be created during instructor configuration", "info")
}

# Validate transcript files
if (length(transcript_files) > 0) {
  log_progress("Validating transcript files...", "info")
  
  valid_transcripts <- c()
  for (file in transcript_files) {
    tryCatch({
      # Quick validation - try to read first few lines
      test_data <- load_zoom_transcript(file)
      if (nrow(test_data) > 0) {
        valid_transcripts <- c(valid_transcripts, file)
        log_progress(sprintf("‚úÖ Valid transcript: %s (%d utterances)", 
                           basename(file), nrow(test_data)), "success")
      } else {
        log_progress(sprintf("‚ö†Ô∏è  Empty transcript: %s", basename(file)), "warning")
      }
    }, error = function(e) {
      log_progress(sprintf("‚ùå Invalid transcript: %s (%s)", basename(file), e$message), "error")
    })
  }
  
  if (length(valid_transcripts) > 0) {
    log_progress(sprintf("‚úÖ Found %d valid transcript files", length(valid_transcripts)), "success")
  } else {
    log_progress("‚ùå No valid transcript files found", "error")
  }
} else {
  log_progress("No transcript files found - please add .transcript.vtt files to data/transcripts/", "warning")
}

log_progress("Data preparation completed", "success")
```

## Step 3: Instructor Configuration

Configure instructor names for proper identification:

```{r instructor-configuration}
log_progress("Starting instructor configuration", "info")

# Safe instructor configuration using package utilities
lookup_file <- file.path("data/metadata", "section_names_lookup.csv")
allow_lookup_write <- isTRUE(params$allow_lookup_write)
instructor_name <- params$instructor_name

log_progress(sprintf("Configuring instructor: %s", instructor_name), "info")

# Read existing lookup or create new one
existing_lookup <- read_lookup_safely(lookup_file)
log_progress(sprintf("Current lookup has %d entries", nrow(existing_lookup)), "info")

# Ensure instructor is configured
updated_lookup <- ensure_instructor_rows(existing_lookup, instructor_name)

if (nrow(existing_lookup) != nrow(updated_lookup)) {
  log_progress("Instructor configuration needed", "info")
  
  if (allow_lookup_write) {
    conditionally_write_lookup(updated_lookup, lookup_file, allow_write = TRUE)
    log_progress("‚úÖ Instructor configuration written (transactional with backup)", "success")
  } else {
    log_progress("‚è© Read-only mode: preview only; no file changes made", "info")
    log_progress("Set params: allow_lookup_write = TRUE to enable updates", "info")
  }
} else {
  log_progress("‚úÖ Instructor already configured", "success")
}

# Preview instructor configuration
instructor_entries <- updated_lookup[updated_lookup$participant_type == "instructor", , drop = FALSE]
if (nrow(instructor_entries) > 0) {
  knitr::kable(instructor_entries[, c("transcript_name", "preferred_name", "participant_type")],
               caption = "Configured Instructors")
}

log_progress("Instructor configuration completed", "success")
```

## Step 4: Privacy Configuration and Testing

Configure and test privacy settings:

```{r privacy-configuration}
log_progress("Starting privacy configuration", "info")

# Set privacy level
privacy_level <- params$privacy_level
set_privacy_defaults(privacy_level)
log_progress(sprintf("Privacy level set to: %s", privacy_level), "info")

# Quick privacy test
if (length(transcript_files) > 0) {
  log_progress("Testing privacy configuration...", "info")
  
  tryCatch({
    # Test with first transcript
    test_file <- transcript_files[1]
    test_metrics <- summarize_transcript_metrics(
      transcript_file_path = test_file,
      names_exclude = c("dead_air")
    )
    
    # Check for real names in output
    unique_names <- unique(test_metrics$name)
    real_name_pattern <- "^[A-Z][a-z]+(\\s+[A-Z][a-z]+)*$"
    real_names <- unique_names[grepl(real_name_pattern, unique_names)]
    
    if (length(real_names) > 0) {
      log_progress(sprintf("‚ö†Ô∏è  Found %d real names in output", length(real_names)), "warning")
      if (privacy_level %in% c("ferpa_strict", "ferpa_standard")) {
        log_progress("Privacy level may not be working correctly", "warning")
      }
    } else {
      log_progress("‚úÖ Privacy configuration working correctly", "success")
    }
    
  }, error = function(e) {
    handle_error(e, "Privacy testing")
  })
} else {
  log_progress("No transcript files available for privacy testing", "warning")
}

log_progress("Privacy configuration completed", "success")
```

## Step 5: Incremental Session Processing

Process transcript files incrementally, tracking which sessions have been analyzed:

```{r incremental-processing}
log_progress("Starting incremental session processing", "info")

# Load session tracking
session_tracking_file <- "data/metadata/session_tracking.csv"
if (file.exists(session_tracking_file)) {
  session_tracking <- read.csv(session_tracking_file, stringsAsFactors = FALSE)
  log_progress(sprintf("Loaded session tracking with %d processed sessions", nrow(session_tracking)), "info")
} else {
  session_tracking <- data.frame(
    session_id = character(),
    file_path = character(),
    processed_date = character(),
    status = character(),
    stringsAsFactors = FALSE
  )
  log_progress("Created new session tracking", "info")
}

# Get all transcript files
transcript_files <- list.files(
  "data/transcripts",
  pattern = "\\.transcript\\.vtt$",
  full.names = TRUE,
  recursive = TRUE
)

if (length(transcript_files) == 0) {
  log_progress("No transcript files found to process", "warning")
} else {
  log_progress(sprintf("Found %d transcript files to process", length(transcript_files)), "info")
  
  # Process each file
  processed_data <- list()
  new_sessions <- 0
  
  for (file_path in transcript_files) {
    session_id <- tools::file_path_sans_ext(basename(file_path))
    
    # Check if already processed
    if (session_id %in% session_tracking$session_id && params$incremental_mode) {
      log_progress(sprintf("‚è© Skipping already processed session: %s", session_id), "info")
      next
    }
    
    log_progress(sprintf("Processing session: %s", session_id), "info")
    
    tryCatch({
      # Process transcript
      transcript_data <- load_zoom_transcript(file_path)
      
      # Classify participants if roster is available
      if (exists("roster")) {
        lookup_data <- read_lookup_safely("data/metadata/section_names_lookup.csv")
        classified_data <- classify_participants(
          transcript_df = transcript_data,
          roster_df = roster,
          lookup_df = lookup_data,
          privacy_level = privacy_level
        )
        
        # Calculate metrics
        session_metrics <- summarize_transcript_metrics(
          transcript_df = classified_data,
          names_exclude = c("dead_air")
        )
        
        # Store processed data
        processed_data[[session_id]] <- list(
          transcript = classified_data,
          metrics = session_metrics,
          session_id = session_id,
          file_path = file_path,
          processed_date = as.character(Sys.Date())
        )
        
        # Update session tracking
        new_row <- data.frame(
          session_id = session_id,
          file_path = file_path,
          processed_date = as.character(Sys.Date()),
          status = "completed",
          stringsAsFactors = FALSE
        )
        session_tracking <- rbind(session_tracking, new_row)
        
        new_sessions <- new_sessions + 1
        log_progress(sprintf("‚úÖ Processed session: %s (%d utterances)", 
                           session_id, nrow(classified_data)), "success")
        
      } else {
        log_progress(sprintf("‚ö†Ô∏è  No roster available for session: %s", session_id), "warning")
      }
      
    }, error = function(e) {
      handle_error(e, sprintf("Processing session %s", session_id))
      
      # Mark session as failed
      new_row <- data.frame(
        session_id = session_id,
        file_path = file_path,
        processed_date = as.character(Sys.Date()),
        status = "failed",
        stringsAsFactors = FALSE
      )
      session_tracking <- rbind(session_tracking, new_row)
    })
  }
  
  # Save updated session tracking
  write.csv(session_tracking, session_tracking_file, row.names = FALSE)
  
  if (new_sessions > 0) {
    log_progress(sprintf("‚úÖ Processed %d new sessions", new_sessions), "success")
  } else {
    log_progress("No new sessions to process", "info")
  }
}

log_progress("Incremental processing completed", "success")
```

## Step 6: Data Analysis and Visualization

Analyze the processed data and create visualizations:

```{r data-analysis}
log_progress("Starting data analysis and visualization", "info")

# Combine all processed data
if (length(processed_data) > 0) {
  log_progress("Combining processed session data...", "info")
  
  # Combine metrics from all sessions
  all_metrics <- do.call(rbind, lapply(processed_data, function(x) {
    x$metrics$session_id <- x$session_id
    x$metrics
  }))
  
  # Combine transcript data
  all_transcripts <- do.call(rbind, lapply(processed_data, function(x) {
    x$transcript$session_id <- x$session_id
    x$transcript
  }))
  
  log_progress(sprintf("Combined data: %d metrics, %d transcript entries", 
                      nrow(all_metrics), nrow(all_transcripts)), "success")
  
  # Create student summary
  if (nrow(all_metrics) > 0) {
    student_summary <- all_metrics %>%
      group_by(name) %>%
      summarise(
        total_utterances = n(),
        total_duration = sum(duration, na.rm = TRUE),
        avg_duration = mean(duration, na.rm = TRUE),
        total_words = sum(wordcount, na.rm = TRUE),
        sessions_participated = n_distinct(session_id),
        .groups = 'drop'
      ) %>%
      arrange(desc(total_utterances))
    
    log_progress(sprintf("Created summary for %d participants", nrow(student_summary)), "success")
    
    # Show summary
    knitr::kable(head(student_summary, 10), caption = "Student Participation Summary")
    
    # Create visualizations
    if (nrow(student_summary) > 0) {
      # Participation by utterance count
      p1 <- ggplot(student_summary, aes(x = reorder(name, total_utterances), y = total_utterances)) +
        geom_bar(stat = "identity", fill = "steelblue") +
        coord_flip() +
        labs(
          title = "Student Participation by Utterance Count",
          x = "Student Name",
          y = "Number of Utterances"
        ) +
        theme_minimal()
      
      print(p1)
      
      # Participation by duration
      p2 <- ggplot(student_summary, aes(x = reorder(name, total_duration), y = total_duration / 60)) +
        geom_bar(stat = "identity", fill = "darkgreen") +
        coord_flip() +
        labs(
          title = "Student Participation by Speaking Time",
          x = "Student Name",
          y = "Speaking Time (minutes)"
        ) +
        theme_minimal()
      
      print(p2)
      
      # Save plots
      ggsave("reports/participation_by_utterances.png", p1, width = 10, height = 8)
      ggsave("reports/participation_by_duration.png", p2, width = 10, height = 8)
      
      log_progress("Created and saved visualizations", "success")
    }
  }
  
} else {
  log_progress("No processed data available for analysis", "warning")
}

log_progress("Data analysis completed", "success")
```

## Step 7: Results Export and Reporting

Export results and generate reports:

```{r results-export}
log_progress("Starting results export and reporting", "info")

# Create output directories
ensure_directory("outputs")
ensure_directory("reports")

# Export processed data
if (length(processed_data) > 0) {
  # Export combined metrics
  if (exists("all_metrics") && nrow(all_metrics) > 0) {
    write_metrics(all_metrics, what = "engagement", 
                 path = "outputs/combined_engagement_metrics.csv", 
                 comments_format = "text")
    log_progress("Exported combined engagement metrics", "success")
  }
  
  # Export student summary
  if (exists("student_summary") && nrow(student_summary) > 0) {
    write_metrics(student_summary, what = "engagement", 
                 path = "outputs/student_participation_summary.csv")
    log_progress("Exported student participation summary", "success")
  }
  
  # Export session tracking
  if (exists("session_tracking") && nrow(session_tracking) > 0) {
    write.csv(session_tracking, "outputs/session_tracking.csv", row.names = FALSE)
    log_progress("Exported session tracking", "success")
  }
  
  # Export workflow configuration
  if (exists("config")) {
    jsonlite::write_json(config, "outputs/workflow_config.json", pretty = TRUE)
    log_progress("Exported workflow configuration", "success")
  }
}

# Generate summary report
log_progress("Generating summary report...", "info")

report_summary <- list(
  workflow_info = list(
    course_id = params$course_id,
    semester = params$semester,
    instructor_name = params$instructor_name,
    privacy_level = params$privacy_level,
    run_date = as.character(Sys.Date()),
    run_time = format(Sys.time(), "%H:%M:%S")
  ),
  data_summary = list(
    total_sessions = length(processed_data),
    total_utterances = if(exists("all_transcripts")) nrow(all_transcripts) else 0,
    total_participants = if(exists("student_summary")) nrow(student_summary) else 0,
    sessions_processed = if(exists("session_tracking")) sum(session_tracking$status == "completed") else 0
  ),
  errors = workflow_state$errors,
  warnings = workflow_state$warnings
)

# Save summary report
jsonlite::write_json(report_summary, "reports/workflow_summary.json", pretty = TRUE)
log_progress("Generated workflow summary report", "success")

log_progress("Results export completed", "success")
```

## Step 8: Workflow Completion and Next Steps

```{r workflow-completion}
log_progress("Workflow execution completed", "success")

# Calculate execution time
execution_time <- difftime(Sys.time(), workflow_state$start_time, units = "mins")
log_progress(sprintf("Total execution time: %.1f minutes", execution_time), "info")

# Summary of what was accomplished
cat("\n")
cat("üéâ Workflow Execution Summary\n")
cat("============================\n")
cat(sprintf("Course: %s (%s)\n", params$course_id, params$semester))
cat(sprintf("Instructor: %s\n", params$instructor_name))
cat(sprintf("Privacy Level: %s\n", params$privacy_level))
cat(sprintf("Sessions Processed: %d\n", length(processed_data)))
if (exists("student_summary")) {
  cat(sprintf("Participants Analyzed: %d\n", nrow(student_summary)))
}
cat(sprintf("Execution Time: %.1f minutes\n", execution_time))

if (length(workflow_state$errors) > 0) {
  cat(sprintf("Errors Encountered: %d\n", length(workflow_state$errors)))
}

cat("\n")
cat("üìÅ Output Files Created:\n")
cat("- outputs/combined_engagement_metrics.csv\n")
cat("- outputs/student_participation_summary.csv\n")
cat("- outputs/session_tracking.csv\n")
cat("- reports/participation_by_utterances.png\n")
cat("- reports/participation_by_duration.png\n")
cat("- reports/workflow_summary.json\n")

cat("\n")
cat("üîÑ For Next Run:\n")
cat("- Add new transcript files to data/transcripts/\n")
cat("- Run this workflow again - it will only process new sessions\n")
cat("- Check outputs/ for your analysis results\n")
cat("- Review reports/ for visualizations and summaries\n")

cat("\n")
cat("üí° Tips for Ongoing Use:\n")
cat("- Keep your roster.csv updated with current student information\n")
cat("- Add instructor name variations to section_names_lookup.csv as needed\n")
cat("- Monitor session_tracking.csv to see processing history\n")
cat("- Use incremental_mode = TRUE to avoid reprocessing existing sessions\n")

log_progress("Workflow summary completed", "success")
```

## üìö Best Practices and Tips

### Privacy and FERPA Compliance
- **Always use appropriate privacy levels** for your use case
- **Validate privacy compliance** at each step
- **Never share outputs** containing real names
- **Document your privacy approach** for institutional review

### Data Organization
- **Keep transcript files organized** by date/session
- **Maintain consistent naming conventions**
- **Back up your original data files**
- **Document any manual name mappings**

### Ethical Considerations
- **Use data to promote equitable participation**, not surveillance
- **Focus on group patterns** rather than individual performance
- **Respect student privacy** and preferences
- **Share insights constructively** with students when appropriate

### Troubleshooting
- **Check the logs** in the console output for detailed information
- **Review session_tracking.csv** to see which sessions were processed
- **Validate your data files** if you encounter errors
- **Use incremental_mode = FALSE** to reprocess all sessions if needed

### Performance Optimization
- **Use incremental processing** to avoid reprocessing existing sessions
- **Monitor execution time** for large datasets
- **Consider processing sessions in batches** for very large datasets
- **Check available memory** when processing many sessions

Remember: The goal is to create an inclusive learning environment where all students feel comfortable and encouraged to participate in ways that work for them. 